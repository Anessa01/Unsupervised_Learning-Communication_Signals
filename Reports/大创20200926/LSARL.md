# Introduction 引言

近几年来我们见证了可视化数据生产性模型的巨大进步.虽然这些模型曾经局限于单模态,少模态,结构简单和低分辨率的领域,但随着建模和硬件的进步,它们已经拥有了可靠的能力来生成复杂,多模态,高分辨率的图像分布.

直观上来说,在特定领域生成数据的能力需要对所述领域高层次的理解.这个观念具有长期的吸引力,因为原始数据都很便宜,几乎可以从互联网获得无限的资源----而且丰富,因为这些图片包含比典型机器学习分类标签更多的信息.但是,尽管生成模式的进步不可否认,问题仍然存在:它们学习到了什么语义,能被用于表征学习吗?

仅从原始数据获得真正理解几乎难以实现.相反地,无监督学习中最成功的方法是从有监督学习领域发展而来的:一类被称为自监督学习的方法.这类方法通常包括以某种方式更改或者盖住数据的特定方面,通过训练模型来预测或者生成缺失的方面.举例来说,在拟着色(?proposed colorization)的无监督学习方法中,给定的模型是输入图像色彩通道的一个子集,由此训练它预测缺失的通道.

生成模型作为一种无监督学习的方法,为自监督学习提供了一种极具吸引力的选择,因为它训练完之后无需对原始数据进行任何调制即可建立完整的数据分布模型.生成模型在表示学习中的一个应用即是生成对抗网络(generative adversarial networks,GAN).GAN架构中的生成器是 已生成数据的 随机采样潜在变量(即噪音)的 前馈映射(feed-forward mapping);学习信号由已训练的鉴别器(用于鉴别真实的和生成的数据样本)提供,引导生成器的输出遵循数据分布.对抗学习推断器(ALI)和双向生成对抗网络(BiGAN)都是GAN架构的扩展,在标准GAN基础上添加了编码器,以此将真实数据映射到潜在层(latents),即生成器学习到的映射的逆.

在最优判别器的极限下,论文表明确定性 BiGAN 的行为类似于自编码器,最大限度地降低了重建成本 l₀； 然而,重建误差曲面的形状是由参数鉴别器决定的,而不是像误差 l₂这样的简单像素级度量.**由于鉴别器通常是一个功能强大的神经网络,我们希望它能产生一个误差曲面，在重建时强调 “语义” 误差,而不是强调低层次的细节.**

> ​		写到这里我去CSDN上查了一下BigBiGAN,[发现是有现成翻译的](https://blog.csdn.net/tMb8Z9Vdm66wH68VX1/article/details/102550312?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522160052900219725254024634%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=160052900219725254024634&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v3~pc_rank_v2-1-102550312.first_rank_ecpm_v3_pc_rank_v2&utm_term=BigBiGAN&spm=1018.2118.3001.4187)...

论文证明了通过BiGAN或者ALI框架学习的编码器实在ImageNet上学习下游任务的一种有效的视觉表示方法.然而,它使用了DCGAN风格的生成器,无法在这个数据集上生成高质量的图像,因此编码器能够建模的语义也相当有限.在这项工作中,我们再次使用BigGAN作为生成器,这是一个能够捕获ImageNet图像中的许多模式和结构的先进模型,我们的贡献如下:

​	证明了BigBiGAN与ImageNet上无监督表示学习的最先进技术相匹敌.

​	我们为BigBiGAN提出了一个更稳定的联合鉴别器.

​	我们对模型设计选择进行了全面的实证分析和消融研究.

​	我们证明,表示学习目标还有助于无条件生成图像,并展示了无条件生成ImageNet的最先进结果.

引言到此结束,综述性内容就这些,后面有关BigBiGAN的原理太过专业...上面的翻译CSDN上可能做的更好些.



Large Scale Adversarial Representation Learning

大规模对抗表征学习

译文：我把Introduction翻完之后发现CSDN上有现成翻译[[1]](https://blog.csdn.net/a312863063/article/details/95381903)，看过自叹不如，所以就不贴我自己的翻译了。

## 概括

​	文章引言部分先介绍了既有的生成模型GAN及其优化：BiGAN和BigGan

### 	GAN（生成对抗网络）：

​	传统的自监督生成模型VAE的结构可以理解为：输入图像image_i经过编码解码获得输出图像image_o,致力于降低$||image_o-image_i||^2$；

​	而GAN的逻辑为：输入图像image_i经过网络生成image_o,再随机采样一个高斯噪声得到无意义图像image_n经过网络生成image_on,致力于提高机器区分image_o和image_on的正确率。

### 	BiGAN：

​	顾名思义，它是双向的，即把神经网络的编码解码部分拆开，假设编码为$E$,解码为$G$,有意义图像为x，噪音采样为z，x经过编码器获得$E(x)$,z经过解码器获得$G(z)$,那么从“量纲”上看，x和$G(z)$都是图像，z和$E(x)$都是编码，直接让区分器区分（x,E(x)）和（G(z),z），是对传统GAN的一种有效优化手段。

### 	BigGAN：

​	顾名思义，大量训练样本，本篇文章的重点。文章证明了

​	1.大规模训练样本对GAN效果提升显著；

​	2.针对大规模样本的采样，文章提出了一种简单的采样技术：截断技巧。这允许对样本多样性和保真度之间的权衡进行明确、细粒度的控制；

​	3.发现了大规模GAN的特有不稳定性，找到了有效减少这种不稳定性的手段，但不能完全消除，除非牺牲极高性能成本。

### 	BigBiGAN：

​	在大规模训练中采用了双向GAN优化。文章针对这种模型做了大量消融实验（这个我真的不懂，大概概念是控制变量法，找到在大规模训练中可有可无的结构去掉），总之确定了最简单稳定的BigBiGAN架构。

## 总结：

BiGAN相对于GAN的意义在于将编码解码之间的隐式表示内容的价值发挥了出来。

DeepMind公开了本篇文章中各种架构的图像生成器的源码，用GAN实现对通信信号进行生成和表征学习也有人尝试[[2]](https://blog.csdn.net/soulproficiency/article/details/106893049),是肉眼可见日后可行的研究方向之一。

[1]https://blog.csdn.net/a312863063/article/details/95381903

[2]https://blog.csdn.net/soulproficiency/article/details/106893049