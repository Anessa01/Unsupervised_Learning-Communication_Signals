[toc]



# 20210131组内会议

## 自编码器

### 自编码器架构

Encoder,Decoder,loss=MSE.

### 去噪自编码器

![img](https://img-blog.csdn.net/20180907172138475?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI0NDA3NjU3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

### 卷积自编码器

### 正则化

$$
L_2:l=E_{in}+\lambda\sum_j\omega_j^2
$$

<img src="https://img-blog.csdn.net/20180621085848886?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3JlZF9zdG9uZTE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述" style="zoom:6000%;" />

dropout
$$
L1:l=E_in+\lambda\sum_j|\omega_j|
$$
<img src="https://img-blog.csdn.net/20180621090333882?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3JlZF9zdG9uZTE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述" style="zoom:6000%;" />

### 网络架构

<img src="C:\Users\31086\AppData\Roaming\Typora\typora-user-images\image-20210130101708191.png" alt="image-20210130101708191" style="zoom:2500%;" />

```python
[b,2,88,1]->

Conv2D(filters=2,kernel_size=(1,40),padding="same")->
relu()->

[2,[b,2,88,1]]->reshape->[b,2*2*88]->

Dense(44,activation=hard-sigmoid)->

[b,44]->

Dense(2*88,activation=sigmoid)->

[b,2*88]->reshape->[b,1,176,1]->

Conv2D(filters=1,kernel_size=(1,81), padding="same")->
relu()->

[b,1,176,1]->reshape->[b,2,88,1]
```

$$
loss=MSE(x_{rec},x)+l1r\times\sum|h|+l2r\times\sum\omega^2
$$

重构结果

![image-20210130102331738](C:\Users\31086\AppData\Roaming\Typora\typora-user-images\image-20210130102331738.png)

![image-20210130102339015](C:\Users\31086\AppData\Roaming\Typora\typora-user-images\image-20210130102339015.png)

编码层

![image-20210130102312831](C:\Users\31086\AppData\Roaming\Typora\typora-user-images\image-20210130102312831.png)

![image-20210130102611829](C:\Users\31086\AppData\Roaming\Typora\typora-user-images\image-20210130102611829.png)

模型权重

![image-20210130102532016](C:\Users\31086\AppData\Roaming\Typora\typora-user-images\image-20210130102532016.png)

![image-20210130102417962](C:\Users\31086\AppData\Roaming\Typora\typora-user-images\image-20210130102417962.png)

## GAN

### 信息熵

**信息量的大小与信息发生的概率成反比**。概率越大，信息量越小。概率越小，信息量越大。
$$
I(x)=-\log P(x)
$$
信息熵用来表示所有信息量的期望:
$$
H(X)=-\sum_{i=1}^nP(x_i)\log P(x_i)
$$

### 相对熵(KL Divergence)

如果对于同一个随机变量X有两个单独的概率分布P和Q,则我们可以使用KL散度来衡量这**两个概率分布之间的差异**。
$$
D_{KL}(p||q)=\sum_ip(x_i)\log\frac{p(x_i)}{q(x_i)}
$$
在机器学习中，常常使用P表示实际分布,Q表示我们期望的它最终满足的分布,那么用KL散度作为损失函数往往可以使P趋近Q的分布.

### 交叉熵

KL散度实际有两项:
$$
D_{KL}(p||q)=\sum_ip\log p-\sum_i p\log q=-H(p)+(-H(p,q))
$$
在梯度下降优化过程中,H(p)项是对逼近pq分布不起作用的,所以也常用交叉熵替代KL散度作为损失函数.

### 生成器$G(z)\rightarrow x_f$

<img src="C:\Users\31086\AppData\Roaming\Typora\typora-user-images\image-20210130103855869.png" alt="image-20210130103855869" style="zoom:2500%;" />

### 判别器$D(x)\rightarrow p$

<img src="C:\Users\31086\AppData\Roaming\Typora\typora-user-images\image-20210130104104431.png" alt="image-20210130104104431" style="zoom:5000%;" />

### 网络对抗

判别器希望$p(x_r)$接近纯1分布,$p(x_f)$接近纯0分布.

生成器希望$p(x_f)$接近纯1分布.
$$
L=-1\times\sum_{x_r}\log D(x_r)+-0\times\sum_{x_f}\log D(x_f)\\
=-1\times\sum_{x_r}\log D(x_r)+-1\times\sum_{x_f}(1-\log D(x_f))\\
=-1\times\sum_{x_r}\log D(x_r)+-1\times\sum_{x_f}(1-\log D(G(z)))
$$
设D网络中参数为$\theta$,G网络中参数为$\phi$,改写上式为minimax形式:
$$
\min_\phi\max_\theta L(D,G)=E\log D_\theta(x)+E[1-\log D_\theta(G_\phi(z))]
$$
纳什均衡状态:$p(x_f)=p(x_r)=0.5$

### DCGAN

![image-20210130105412948](C:\Users\31086\AppData\Roaming\Typora\typora-user-images\image-20210130105412948.png)

![image-20210130105619918](C:\Users\31086\AppData\Roaming\Typora\typora-user-images\image-20210130105619918.png)

<img src="C:\Users\31086\AppData\Roaming\Typora\typora-user-images\image-20210130105456834.png" alt="image-20210130105456834" style="zoom:5000%;" />

### WGAN

当两个分布完全不重叠时，无论分布之间的距离远近，KL散度为恒定值1/2log 2， 此时KL散度将无法产生有效的梯度信息；当两个分布出现重叠时，KL 散度才会平滑变动，产生有效梯度信息;

<img src="C:\Users\31086\AppData\Roaming\Typora\typora-user-images\image-20210130112026950.png" alt="image-20210130112026950" style="zoom:2500%;" />

Wasserstein
$$
W(p,q)=\inf_{\gamma\in\Pi(p,q)}E_{x,y\in\gamma}[||x-y||]
$$
推土机距离,它表示一个分布到另一个分布的最小代价.更适合指导GAN网络的训练.

![wgan-5000](D:\DaChuang\WGAN\images\wgan-5000.png)

### BiGAN

![image-20210130113157763](C:\Users\31086\AppData\Roaming\Typora\typora-user-images\image-20210130113157763.png)

BiGAN 希望让 GAN 能够具备表征学习能力

### BigGAN,BigBiGAN

大量数据,大量参数,大量消融实验,大量训练时间,首次展示了大规模训练对表征学习的益处.