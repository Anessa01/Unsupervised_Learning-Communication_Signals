#### Without BN

###### Why does BN work?

1.solve the problem of covariance shift

Suppose you have trained your cat-recognizing network use black cat, but evaluate on colored cats, you will see **data distribution** changing(called covariance shift). Even there exist a true boundary separate cat and non-cat, you can't expect learn that boundary only with black cat. So you may need to retrain the network.

For a neural network, suppose input distribution is constant, so output distribution of a certain hidden layer should have been constant. But as the weights of that layer and previous layers changing in the training phase, **the output distribution will change**, this cause covariance shift from the perspective of layer after it. Just like cat-recognizing network, the following need to re-train. To recover this problem, we use batch normal to force a zero-mean and one-variance distribution. **It allow layer after it to learn independently from previous layers**, and more concentrate on its own task, and so as to speed up the training process.

2.Batch normal as regularization(slightly)

In batch normal, mean and variance is computed on mini-batch, which consist not too much samples. So the mean and variance contains noise. Just like dropout, it adds some noise to hidden layer's activation(dropout randomly multiply activation by 0 or 1).

### Why not BN?

undesirable properties stemming from its dependence on the batch size and interactions between examples



adaptive gradient clipping technique.





# Modulation Recognition

![image-20210629172020648](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20210629172020648.png)



《Convolutional Radio Modulation Recognition Networks》

[《Convolutional Radio Modulation Recognition Networks》阅读总结和思维导图 - 简书 (jianshu.com)](https://www.jianshu.com/p/1d0b2379db74)



## HOW DO WE APPLY U-Learning

pre-training of the network. Lead to less use of labels or less convergence time.



self-encoder Noise removal.



